import os
import streamlit as st
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_community.document_loaders import TextLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv

# è¿™ä¸€è¡Œä¼šè‡ªåŠ¨å¯»æ‰¾å¹¶åŠ è½½ .env æ–‡ä»¶é‡Œçš„å˜é‡
load_dotenv()
# ----------------- é…ç½®åŒºåŸŸ -----------------
# è¿™é‡Œå¡«å…¥ä½ çš„ API KEY
# å¦‚æœç”¨ OpenAI: OS.environ["OPENAI_API_KEY"] = "sk-..."
# å¦‚æœç”¨ æ™ºè°±GLM (æ¨è):
os.environ["ZHIPUAI_API_KEY"] = "ZHIPUAI_API_KEY"

# é…ç½® LLM å’Œ Embedding
# æ™ºè°±çš„å…¼å®¹æ¥å£åœ°å€æ˜¯ https://open.bigmodel.cn/api/paas/v4/
# æ¨¡å‹ä½¿ç”¨ GLM-4-Flash (é€Ÿåº¦å¿«å…è´¹/ä¾¿å®œ) æˆ– GLM-4
llm = ChatOpenAI(
    temperature=0.3,
    model="glm-4-flash",
    openai_api_base="https://open.bigmodel.cn/api/paas/v4/",
    openai_api_key=os.environ["ZHIPUAI_API_KEY"]
)


# æ™ºè°±çš„ Embedding ç›®å‰ LangChain å…¼å®¹æ€§ç¨å·®ï¼Œè¿™é‡Œç”¨é€šç”¨çš„æˆ–è€… OpenAI æ ¼å¼
# å¦‚æœä¸ºäº†ç®€å•ï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥ä¸´æ—¶ç”¨ huggingface çš„å¼€æºæ¨¡å‹ï¼Œæˆ–è€…ç›´æ¥ç”¨ OpenAI çš„æ¥å£æ ¼å¼è°ƒç”¨æ™ºè°± embedding
# ä¸ºäº†æ¼”ç¤ºæœ€ç®€ä¾¿çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‡è®¾ä½ ç”¨çš„æ˜¯æ™ºè°±çš„æ ‡å‡† embedding (éœ€è¦å®‰è£… zhipuai åº“)
# ä½†ä¸ºé™ä½é—¨æ§›ï¼Œè¿™é‡Œæ¼”ç¤ºä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼ï¼ˆæˆ–è€…å¦‚æœä½ æœ‰ OpenAI Key ç›´æ¥ç”¨å³å¯ï¼‰
# ä¸‹é¢å±•ç¤ºæ ‡å‡† LangChain æµç¨‹
# -------------------------------------------

@st.cache_resource
def init_knowledge_base():
    """
    åˆå§‹åŒ–çŸ¥è¯†åº“ï¼šè¯»å–txt -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å…¥å‘é‡æ•°æ®åº“
    ä½¿ç”¨ @st.cache_resource ä¿è¯åªæœ‰ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶åŠ è½½ï¼Œä¹‹åç›´æ¥è¯»å–ç¼“å­˜
    """
    # 1. åŠ è½½æ•°æ®
    if not os.path.exists("chu_knowledge.txt"):
        return None

    loader = TextLoader("chu_knowledge.txt", encoding="utf-8")
    docs = loader.load()

    # 2. åˆ‡åˆ†æ–‡æœ¬ (Chunks)
    # æŠŠé•¿æ–‡ç« åˆ‡æˆå°å—ï¼Œæ–¹ä¾¿æ£€ç´¢
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    # 3. å‘é‡åŒ– (Embeddings)
    # æ³¨æ„ï¼šå¦‚æœæ²¡æœ‰ OpenAI é¢åº¦ï¼Œå¯ä»¥ä½¿ç”¨ 'sentence-transformers' (å…è´¹æœ¬åœ°æ¨¡å‹)
    # è¿™é‡Œæ¼”ç¤ºä½¿ç”¨ ZhipuAI çš„ Embedding (éœ€è‡ªå®šä¹‰æˆ–ä½¿ç”¨å…¼å®¹å±‚)ï¼Œ
    # ä¸ºç®€åŒ–ä»£ç ï¼Œæ­¤å¤„å‡è®¾ä½ ä½¿ç”¨ OpenAI æˆ– æ™ºè°±å…¼å®¹çš„ Embedding æ¥å£
    embeddings = OpenAIEmbeddings(
        model="embedding-2",  # æ™ºè°±çš„ embedding æ¨¡å‹å
        openai_api_base="https://open.bigmodel.cn/api/paas/v4/",
        openai_api_key=os.environ["ZHIPUAI_API_KEY"]
    )

    # 4. å­˜å…¥ Chroma å‘é‡æ•°æ®åº“ (å†…å­˜æ¨¡å¼)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    return vectorstore


# åˆå§‹åŒ–ç•Œé¢
st.title("ğŸ›¡ï¸ æ¥šæ–‡åŒ–æ™ºèƒ½é—®ç­”åŠ©æ‰‹")
st.markdown("åŸºäº **LangChain + GLM** | ä¸“æ³¨äºæ¥šç³»æ–‡å­—ä¸è€ƒå¤çŸ¥è¯†")

# ä¾§è¾¹æ 
with st.sidebar:
    st.write("ğŸ“– **çŸ¥è¯†åº“çŠ¶æ€**")
    if os.path.exists("chu_knowledge.txt"):
        st.success("çŸ¥è¯†åº“æ–‡ä»¶å·²æ£€æµ‹åˆ°")
        if st.button("ğŸ”„ é‡å»º/æ›´æ–°çŸ¥è¯†åº“"):
            st.cache_resource.clear()
            st.rerun()
    else:
        st.error("è¯·åœ¨æ ¹ç›®å½•åˆ›å»º chu_knowledge.txt å¹¶æ”¾å…¥èµ„æ–™")

# åŠ è½½çŸ¥è¯†åº“
vectorstore = init_knowledge_base()

if vectorstore:
    # 1. å®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼ˆè¿™ä¸€æ­¥å¯ä»¥è®© AI æ‰®æ¼”ç‰¹å®šè§’è‰²ï¼‰
    prompt_template = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªè€ƒå¤å­¦ä¸“å®¶ï¼Œè¯·ç”¨é€šä¿—æ˜“æ‡‚ä½†ä¸¥è°¨çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä½ ä¸“é—¨ç ”ç©¶æ¥šç³»æ–‡åŒ–ï¼ˆåŒ…æ‹¬é’é“œå™¨ã€ç®€å¸›æ–‡å­—ã€æ¼†æœ¨å™¨åŠæˆ˜å›½å†å²ï¼‰ã€‚
        
        ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„ã€å·²çŸ¥ä¿¡æ¯ã€‘ï¼ˆContextï¼‰æ¥å›ç­”ç”¨æˆ·çš„æé—®ã€‚è¯·éµå¾ªä»¥ä¸‹å‡†åˆ™ï¼š
        
        1.  **è§’è‰²è®¾å®š**ï¼šä½ å°±åƒä¸€ä½çŸ¥è¯†æ¸Šåšçš„åšç‰©é¦†é‡‘ç‰Œè®²è§£å‘˜ã€‚é¢å¯¹ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚â€œé¸Ÿè™«ä¹¦â€ã€â€œå¤±èœ¡æ³•â€ã€â€œæ‚¬å±±é¡¶â€ï¼‰ï¼Œå°½é‡ç”¨ç°ä»£ç”Ÿæ´»ä¸­çš„ç±»æ¯”æˆ–é€šä¿—è¯­è¨€è¿›è¡Œè§£é‡Šï¼Œä½†å¿…é¡»ä¿æŒå†å²äº‹å®çš„å‡†ç¡®æ€§ã€‚
        2.  **ä¾æ®äº‹å®**ï¼šè¯·ä¸¥æ ¼åŸºäºã€å·²çŸ¥ä¿¡æ¯ã€‘å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸­åŒ…å«å…·ä½“çš„å‡ºåœŸå¹´ä»£ã€åœ°ç‚¹æˆ–å°ºå¯¸æ•°æ®ï¼Œè¯·åŠ¡å¿…å¼•ç”¨ä»¥å¢åŠ å¯ä¿¡åº¦ã€‚
        3.  **è¯šå®åŸåˆ™**ï¼šå¦‚æœã€å·²çŸ¥ä¿¡æ¯ã€‘ä¸­æ²¡æœ‰åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„çŸ¥è¯†ï¼Œè¯·ç›´æ¥å‘ŠçŸ¥ç”¨æˆ·ï¼šâ€œæŠ±æ­‰ï¼Œç›®å‰çš„è€ƒå¤èµ„æ–™åº“ä¸­æš‚æ— æ­¤è®°å½•â€ï¼Œä¸¥ç¦è‡†æµ‹æˆ–ç¼–é€ å†å²äº‹å®ã€‚
        4.  **å›ç­”ç»“æ„**ï¼š
            *   å…ˆç›´æ¥ç»™å‡ºæ ¸å¿ƒç»“è®ºã€‚
            *   å†å±•å¼€è¯¦ç»†æè¿°ï¼ˆæ–‡ç‰©çš„å½¢åˆ¶ã€çº¹é¥°ã€å†å²èƒŒæ™¯ï¼‰ã€‚
            *   æœ€åï¼ˆå¦‚æœç›¸å…³ï¼‰å¯ä»¥å»¶ä¼¸ä¸€ä¸¤å¥è¯¥æ–‡ç‰©åœ¨æ¥šæ–‡åŒ–ä¸­çš„ç‹¬ç‰¹åœ°ä½æˆ–å®¡ç¾ä»·å€¼ã€‚
        5.  **è¯­æ°”é£æ ¼**ï¼šå®¢è§‚ã€å…¸é›…ã€å¼•äººå…¥èƒœã€‚ä¸è¦ä½¿ç”¨è¿‡äºåƒµç¡¬çš„æœºå™¨ç¿»è¯‘è…”ï¼Œä¹Ÿä¸è¦ä½¿ç”¨è½»æµ®çš„ç½‘ç»œç”¨è¯­ã€‚
        
        ã€å·²çŸ¥ä¿¡æ¯ã€‘ï¼š
        {context}
        
        ç”¨æˆ·é—®é¢˜ï¼š
        {question}
       """)

    # 2. åˆ›å»ºæ–‡æ¡£å¤„ç†é“¾ï¼ˆStuffé“¾ï¼šæŠŠæ£€ç´¢åˆ°çš„æ–‡æ¡£å¡è¿› promptï¼‰
    document_chain = create_stuff_documents_chain(llm, prompt_template)

    # 3. åˆ›å»ºæ£€ç´¢é“¾ï¼ˆæŠŠ æ£€ç´¢å™¨ å’Œ æ–‡æ¡£é“¾ è¿èµ·æ¥ï¼‰
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    qa_chain = create_retrieval_chain(retriever, document_chain)

    # èŠå¤©ç•Œé¢
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²è®°å½•
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥å…³äºæ¥šæ–‡åŒ–çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯é¸Ÿè™«ä¹¦ï¼Ÿ"):
        # 1. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # 2. è°ƒç”¨ AI å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢è€ƒå¤èµ„æ–™åº“..."):
                response = qa_chain.invoke({"query": prompt})
                answer = response["result"]
                source_docs = response["source_documents"]

                st.write(answer)

                # (å¯é€‰) æ˜¾ç¤ºå‚è€ƒæ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦
                with st.expander("ğŸ“š å‚è€ƒèµ„æ–™æ¥æº"):
                    for doc in source_docs:
                        st.caption(f"...{doc.page_content}...")

        # 3. ä¿å­˜ AI å›ç­”
        st.session_state.messages.append({"role": "assistant", "content": answer})

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨ä¾§è¾¹æ ç¡®è®¤çŸ¥è¯†åº“æ–‡ä»¶å·²å°±ç»ªã€‚")