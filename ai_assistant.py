import os
import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

# ----------------- é…ç½®åŒºåŸŸ -----------------
# è¿™é‡Œå¡«å…¥ä½ çš„ API KEY
# å¦‚æœç”¨ OpenAI: OS.environ["OPENAI_API_KEY"] = "sk-..."
# å¦‚æœç”¨ æ™ºè°±GLM (æ¨è):
os.environ["ZHIPUAI_API_KEY"] = "ä½ çš„_ZHIPU_API_KEY"

# é…ç½® LLM å’Œ Embedding
# æ™ºè°±çš„å…¼å®¹æ¥å£åœ°å€æ˜¯ https://open.bigmodel.cn/api/paas/v4/
# æ¨¡å‹ä½¿ç”¨ GLM-4-Flash (é€Ÿåº¦å¿«å…è´¹/ä¾¿å®œ) æˆ– GLM-4
llm = ChatOpenAI(
    temperature=0.3,
    model="glm-4-flash",
    openai_api_base="https://open.bigmodel.cn/api/paas/v4/",
    openai_api_key=os.environ["ZHIPUAI_API_KEY"]
)


# æ™ºè°±çš„ Embedding ç›®å‰ LangChain å…¼å®¹æ€§ç¨å·®ï¼Œè¿™é‡Œç”¨é€šç”¨çš„æˆ–è€… OpenAI æ ¼å¼
# å¦‚æœä¸ºäº†ç®€å•ï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥ä¸´æ—¶ç”¨ huggingface çš„å¼€æºæ¨¡å‹ï¼Œæˆ–è€…ç›´æ¥ç”¨ OpenAI çš„æ¥å£æ ¼å¼è°ƒç”¨æ™ºè°± embedding
# ä¸ºäº†æ¼”ç¤ºæœ€ç®€ä¾¿çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‡è®¾ä½ ç”¨çš„æ˜¯æ™ºè°±çš„æ ‡å‡† embedding (éœ€è¦å®‰è£… zhipuai åº“)
# ä½†ä¸ºé™ä½é—¨æ§›ï¼Œè¿™é‡Œæ¼”ç¤ºä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼ï¼ˆæˆ–è€…å¦‚æœä½ æœ‰ OpenAI Key ç›´æ¥ç”¨å³å¯ï¼‰
# ä¸‹é¢å±•ç¤ºæ ‡å‡† LangChain æµç¨‹
# -------------------------------------------

@st.cache_resource
def init_knowledge_base():
    """
    åˆå§‹åŒ–çŸ¥è¯†åº“ï¼šè¯»å–txt -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å…¥å‘é‡æ•°æ®åº“
    ä½¿ç”¨ @st.cache_resource ä¿è¯åªæœ‰ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶åŠ è½½ï¼Œä¹‹åç›´æ¥è¯»å–ç¼“å­˜
    """
    # 1. åŠ è½½æ•°æ®
    if not os.path.exists("chu_knowledge.txt"):
        return None

    loader = TextLoader("chu_knowledge.txt", encoding="utf-8")
    docs = loader.load()

    # 2. åˆ‡åˆ†æ–‡æœ¬ (Chunks)
    # æŠŠé•¿æ–‡ç« åˆ‡æˆå°å—ï¼Œæ–¹ä¾¿æ£€ç´¢
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    # 3. å‘é‡åŒ– (Embeddings)
    # æ³¨æ„ï¼šå¦‚æœæ²¡æœ‰ OpenAI é¢åº¦ï¼Œå¯ä»¥ä½¿ç”¨ 'sentence-transformers' (å…è´¹æœ¬åœ°æ¨¡å‹)
    # è¿™é‡Œæ¼”ç¤ºä½¿ç”¨ ZhipuAI çš„ Embedding (éœ€è‡ªå®šä¹‰æˆ–ä½¿ç”¨å…¼å®¹å±‚)ï¼Œ
    # ä¸ºç®€åŒ–ä»£ç ï¼Œæ­¤å¤„å‡è®¾ä½ ä½¿ç”¨ OpenAI æˆ– æ™ºè°±å…¼å®¹çš„ Embedding æ¥å£
    embeddings = OpenAIEmbeddings(
        model="embedding-2",  # æ™ºè°±çš„ embedding æ¨¡å‹å
        openai_api_base="https://open.bigmodel.cn/api/paas/v4/",
        openai_api_key=os.environ["ZHIPUAI_API_KEY"]
    )

    # 4. å­˜å…¥ Chroma å‘é‡æ•°æ®åº“ (å†…å­˜æ¨¡å¼)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    return vectorstore


# åˆå§‹åŒ–ç•Œé¢
st.title("ğŸ›¡ï¸ æ¥šæ–‡åŒ–æ™ºèƒ½é—®ç­”åŠ©æ‰‹")
st.markdown("åŸºäº **LangChain + GLM** | ä¸“æ³¨äºæ¥šç³»æ–‡å­—ä¸è€ƒå¤çŸ¥è¯†")

# ä¾§è¾¹æ 
with st.sidebar:
    st.write("ğŸ“– **çŸ¥è¯†åº“çŠ¶æ€**")
    if os.path.exists("chu_knowledge.txt"):
        st.success("çŸ¥è¯†åº“æ–‡ä»¶å·²æ£€æµ‹åˆ°")
        if st.button("ğŸ”„ é‡å»º/æ›´æ–°çŸ¥è¯†åº“"):
            st.cache_resource.clear()
            st.rerun()
    else:
        st.error("è¯·åœ¨æ ¹ç›®å½•åˆ›å»º chu_knowledge.txt å¹¶æ”¾å…¥èµ„æ–™")

# åŠ è½½çŸ¥è¯†åº“
vectorstore = init_knowledge_base()

if vectorstore:
    # åˆ›å»ºæ£€ç´¢é“¾
    # k=3 è¡¨ç¤ºæ¯æ¬¡æ‰¾ 3 æ¡æœ€ç›¸å…³çš„èµ„æ–™
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )

    # èŠå¤©ç•Œé¢
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²è®°å½•
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥å…³äºæ¥šæ–‡åŒ–çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯é¸Ÿè™«ä¹¦ï¼Ÿ"):
        # 1. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # 2. è°ƒç”¨ AI å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢è€ƒå¤èµ„æ–™åº“..."):
                response = qa_chain.invoke({"query": prompt})
                answer = response["result"]
                source_docs = response["source_documents"]

                st.write(answer)

                # (å¯é€‰) æ˜¾ç¤ºå‚è€ƒæ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦
                with st.expander("ğŸ“š å‚è€ƒèµ„æ–™æ¥æº"):
                    for doc in source_docs:
                        st.caption(f"...{doc.page_content}...")

        # 3. ä¿å­˜ AI å›ç­”
        st.session_state.messages.append({"role": "assistant", "content": answer})

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨ä¾§è¾¹æ ç¡®è®¤çŸ¥è¯†åº“æ–‡ä»¶å·²å°±ç»ªã€‚")